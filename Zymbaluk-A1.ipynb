{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\newcommand{\\xv}{\\mathbf{x}}\n",
    "\\newcommand{\\Xv}{\\mathbf{X}}\n",
    "\\newcommand{\\yv}{\\mathbf{y}}\n",
    "\\newcommand{\\zv}{\\mathbf{z}}\n",
    "\\newcommand{\\av}{\\mathbf{a}}\n",
    "\\newcommand{\\Wv}{\\mathbf{W}}\n",
    "\\newcommand{\\wv}{\\mathbf{w}}\n",
    "\\newcommand{\\tv}{\\mathbf{t}}\n",
    "\\newcommand{\\Tv}{\\mathbf{T}}\n",
    "\\newcommand{\\muv}{\\boldsymbol{\\mu}}\n",
    "\\newcommand{\\sigmav}{\\boldsymbol{\\sigma}}\n",
    "\\newcommand{\\phiv}{\\boldsymbol{\\phi}}\n",
    "\\newcommand{\\Phiv}{\\boldsymbol{\\Phi}}\n",
    "\\newcommand{\\Sigmav}{\\boldsymbol{\\Sigma}}\n",
    "\\newcommand{\\Lambdav}{\\boldsymbol{\\Lambda}}\n",
    "\\newcommand{\\half}{\\frac{1}{2}}\n",
    "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
    "\\newcommand{\\argmin}[1]{\\underset{#1}{\\operatorname{argmin}}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jared Zymbaluk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this assignment was to represent a linear regression model in Python code. We do this by choosing the variables that we would like to predict, and the rest of the variables become the the dataset that we use to predict these variables. We do this by standardizing our inputs, and through a process of matrix multiplication. We can calculate the RMSE from our predictions. This RMSE represents how far off our predictions are from the actual data points. Therefore, when we \"minimize the sum of squared errors\" we get closer and closer to the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, T):\n",
    "    #Code taken from Professor Anderson's notebook\n",
    "    means = X.mean(0)\n",
    "    stds = X.std(0)\n",
    "    Xs = (X - means) / stds\n",
    "    Xs1 = np.insert(Xs, 0, 1, 1)               \n",
    "    w = np.linalg.lstsq( Xs1.T @ Xs1, Xs1.T @ T)[0]\n",
    "    \n",
    "    #build return dictionary\n",
    "    return {\"means\" : means,\n",
    "             \"stds\" : stds,\n",
    "             \"w\" : w\n",
    "           }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def use(model, X):\n",
    "    #Code taken from Professor Anderson's notebook\n",
    "    newX = (X - model[\"means\"]) / model[\"stds\"]\n",
    "    \n",
    "    #insert columns into array\n",
    "    newX = np.insert(newX, 0, 1, 1)\n",
    "    \n",
    "    #make prediction\n",
    "    prediction = newX @ model[\"w\"]\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rmse(predict, T):\n",
    "    #find average of the difference, square it and then square root\n",
    "    return np.sqrt(np.mean((predict -  T)**2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def trainSGD(X, T, learningRate, numberOfIterations):\n",
    "    #Code taken from Professor Anderson's notebook\n",
    "    means = X.mean(0)\n",
    "    stds = X.std(0)\n",
    "    Xs = (X - means) / stds\n",
    "    Xs1 = np.insert(Xs, 0, 1, axis=1)\n",
    "    \n",
    "    w = np.zeros((Xs1.shape[1],T.shape[1]))\n",
    "    nOutputs = T.shape[1]\n",
    "    nInputs = Xs1.shape[1]\n",
    "    \n",
    "    for iter in range(numberOfIterations):\n",
    "        for n in range(len(X)):\n",
    "            predicted = Xs1[n:n+1,:] @ w\n",
    "            w += learningRate * Xs1[n:n+1, :].T * (T[n:n+1, :] - predicted)\n",
    "        \n",
    "                        \n",
    "    return {\"means\" : means,\n",
    "           \"stds\" : stds,\n",
    "           \"w\" : w\n",
    "          }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download ```energydata_complete.csv``` from the [Appliances energy prediction Data Set ](https://archive.ics.uci.edu/ml/datasets/Appliances+energy+prediction) at the UCI Machine Learning Repository. Ignore the first column (date and time), use the next two columns as target variables, and use all but the last two columns (named rv1 and rv2) as input variables. \n",
    "\n",
    "There are 19735 total data instances, Appliances and lights are the variables we want to predict. The other variables are our training variables. The data was collected by Luis Candanedo with a ZigBee wireless sensor network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = np.genfromtxt('energydata_complete.csv', dtype='str',delimiter=',',deletechars='\"')\n",
    "file = np.char.replace(file, '\"', '')\n",
    "file = np.char.replace(file, ' ', '')\n",
    "file = np.delete(file, 0,1)\n",
    "file = np.delete(file, -1,1)\n",
    "file = np.delete(file, -1,1)\n",
    "names = file[0]\n",
    "names = names.astype(np.str)\n",
    "data = file[1:]\n",
    "data = data.astype(np.float)\n",
    "\n",
    "Tenergy = np.take(data,[0,1],1)\n",
    "Xenergy = np.take(data,range(2,26),1)\n",
    "Tnames = np.take(names,[0,1])\n",
    "Xnames = np.take(names,range(2,26))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.plot(Xenergy, Tenergy, 'o')\n",
    "plt.xlabel(\"Predictor Variables\")\n",
    "plt.ylabel(\"Target Variables\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply your functions to the data.  Compare the error you get as a result of both training functions.  Experiment with different learning rates for ```trainSGD``` and discuss the errors.\n",
    "\n",
    "Make some plots of the predicted energy uses and the actual energy uses versus the sample index.  Also plot predicted energy use versus actual energy use.  Show the above plots for the appliances energy use and repeat them for the lights energy use. Discuss your observations of each graph.\n",
    "\n",
    "Show the values of the resulting weights and discuss which ones might be least relevant for fitting your linear model.  Remove them, fit the linear model again, plot the results, and discuss what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with regular Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained = train(Xenergy,Tenergy)\n",
    "used = use(trained, Xenergy)\n",
    "rmse1 = rmse(used,Tenergy)\n",
    "rmse1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Training with SGD LR 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedSGD = trainSGD(Xenergy,Tenergy, .01, 10)\n",
    "usedSGD = use(trainedSGD, Xenergy)\n",
    "rmse2 = rmse(usedSGD,Tenergy)\n",
    "rmse2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^This is bad, as we can see later on, our predictions are way off. Let's try it with a smaller learning rate!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with SGD LR 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedSGD3 = trainSGD(Xenergy,Tenergy, .001, 100)\n",
    "usedSGD3 = use(trainedSGD3, Xenergy)\n",
    "rmse3 = rmse(usedSGD3,Tenergy)\n",
    "rmse3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^Getting closer! Let's go even smaller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with SGD LR 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedSGD4 = trainSGD(Xenergy,Tenergy, .00001, 100)\n",
    "usedSGD4 = use(trainedSGD4, Xenergy)\n",
    "rmse4 = rmse(usedSGD4,Tenergy)\n",
    "rmse4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^This is pretty good! any smaller and we will get a less accurate prediction. We'll stick with this one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting a sample index from 1-100 (train and best RMSE of trainSGD):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    for i in range(100):\n",
    "        plt.subplot(2, 1, p+1)\n",
    "        plt.plot(i, Tenergy[i, p], 'or',)\n",
    "        plt.plot(i, used[i, p], 'ob',)\n",
    "        plt.xlabel(\"index\")\n",
    "        plt.ylabel(\"usage\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^It looks like our prediction for the first column of data was actually pretty good. It seems like a few outliers might have thrown off our results. However, the general curve of the line is fairly similar. The second column is less accurate. I would think it is because the amount of zeros from indexes 50-80 might have thrown off our predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    for i in range(100):\n",
    "        plt.subplot(2, 1, p+1)\n",
    "        plt.plot(i, Tenergy[i, p], 'or',)\n",
    "        plt.plot(i, usedSGD4[i, p], 'ob',)\n",
    "        plt.xlabel(\"index\")\n",
    "        plt.ylabel(\"usage\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^these plots appear fairly similar to our regular train plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting predicted energy use vs. actual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plotting regular train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    plt.subplot(2, 1, p+1)\n",
    "    plt.plot(used[:, p], Tenergy[:, p], 'o')\n",
    "    plt.xlabel(\"Predicted \")\n",
    "    plt.ylabel(\"Actual \" )\n",
    "    a = max(min(used[:, p]), min(Tenergy[:, p]))\n",
    "    b = min(max(used[:, p]), max(Tenergy[:, p]))\n",
    "    plt.plot([a, b], [a, b], 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LR .01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    plt.subplot(2, 1, p+1)\n",
    "    plt.plot(usedSGD[:, p], Tenergy[:, p], 'o')\n",
    "    plt.xlabel(\"Predicted \")\n",
    "    plt.ylabel(\"Actual \" )\n",
    "    a = max(min(usedSGD[:, p]), min(Tenergy[:, p]))\n",
    "    b = min(max(usedSGD[:, p]), max(Tenergy[:, p]))\n",
    "    plt.plot([a, b], [a, b], 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LR .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    plt.subplot(2, 1, p+1)\n",
    "    plt.plot(usedSGD3[:, p], Tenergy[:, p], 'o')\n",
    "    plt.xlabel(\"Predicted \")\n",
    "    plt.ylabel(\"Actual \" )\n",
    "    a = max(min(usedSGD3[:, p]), min(Tenergy[:, p]))\n",
    "    b = min(max(usedSGD3[:, p]), max(Tenergy[:, p]))\n",
    "    plt.plot([a, b], [a, b], 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting LR .001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "for p in range(2):\n",
    "    plt.subplot(2, 1, p+1)\n",
    "    plt.plot(usedSGD4[:, p], Tenergy[:, p], 'o')\n",
    "    plt.xlabel(\"Predicted \")\n",
    "    plt.ylabel(\"Actual \" )\n",
    "    a = max(min(usedSGD4[:, p]), min(Tenergy[:, p]))\n",
    "    b = min(max(usedSGD4[:, p]), max(Tenergy[:, p]))\n",
    "    plt.plot([a, b], [a, b], 'r', linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trained[\"w\"])\n",
    "print(trainedSGD4[\"w\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^We can see that some of these have a very low weight! Let's try pruning some to see if we can do better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I was unable to figure out how to remove these weights from w, and get it to still work with the use function. I realize that if we were to remove the low weights from our array, we would get a better prediction"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
